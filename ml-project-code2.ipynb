{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import fmin, tpe, hp, anneal, Trials\n",
    "import hyperopt\n",
    "import gc\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "usecols = ['fecha_dato', 'ncodpers', 'ind_empleado', 'pais_residencia', 'sexo',\n",
    "        'age', 'fecha_alta', 'ind_nuevo', 'antiguedad', 'indrel',\n",
    "        'indrel_1mes', 'tiprel_1mes', 'indresi', 'indext',\n",
    "        'canal_entrada', 'indfall','nomprov', 'ind_actividad_cliente', 'renta', 'segmento',\n",
    "       'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n",
    "       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n",
    "       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n",
    "       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
    "       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n",
    "       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n",
    "       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n",
    "       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n",
    "\n",
    "# USECOLS ARE ALL THE COLUMNS THAT WE ARE USING FOR OUR PROJECT\n",
    "# WE HAVE DROPPED A TOTAL OF 4 COLUMNS WHICH WERE TOO SKEWED.\n",
    "\n",
    "usecols1 = ['fecha_dato', 'ncodpers', 'ind_empleado', 'pais_residencia', 'sexo','age', \n",
    "            'fecha_alta', 'ind_nuevo', 'antiguedad', 'indrel','indrel_1mes', 'tiprel_1mes',\n",
    "        'indresi', 'indext','canal_entrada', 'indfall','nomprov', 'ind_actividad_cliente', 'renta', 'segmento']\n",
    "\n",
    "# THESE ARE ALL THE FEATURE COLUMNS WE ARE USING.\n",
    "\n",
    "usecols2 = ['fecha_dato','ncodpers','ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n",
    "       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n",
    "       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n",
    "       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
    "       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n",
    "       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n",
    "       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n",
    "       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n",
    "\n",
    "# THESE ARE ALL THE PRODUCT COLUMNS ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA LOADING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING OF TRAIN AND TEST DATA IN df and df_t respectively.\n",
    "\n",
    "df = pd.read_csv('../input/santander-pr/train.csv',dtype={\"sexo\":str,\"ind_nuevo\":str,\"ult_fec_cli_1t\":str,\"indext\":str,\"ind_empleado\":\"category\",\"ind_nuevo\":\"category\",\"indrel\":\"category\",\"indrel_1mes\":\"category\",\"tiprel_1mes\":\"category\",\"indresi\":\"category\",\"indext\":\"category\",\"conyuemp\":\"category\",\"indfall\":\"category\",\"canal_entrada\":\"category\",\"ind_ahor_fin_ult1\":\"int8\",'ind_aval_fin_ult1':\"int8\", 'ind_cco_fin_ult1':\"int8\",'ind_cder_fin_ult1':\"int8\", 'ind_cno_fin_ult1':\"int8\", 'ind_ctju_fin_ult1':\"int8\",'ind_ctma_fin_ult1':\"int8\", 'ind_ctop_fin_ult1':\"int8\", 'ind_ctpp_fin_ult1':\"int8\",'ind_deco_fin_ult1':\"int8\", 'ind_deme_fin_ult1':\"int8\", 'ind_dela_fin_ult1':\"int8\",'ind_ecue_fin_ult1':\"int8\", 'ind_fond_fin_ult1':\"int8\", 'ind_hip_fin_ult1':\"int8\",'ind_plan_fin_ult1':\"int8\", 'ind_pres_fin_ult1':\"int8\", 'ind_reca_fin_ult1':\"int8\",'ind_tjcr_fin_ult1':\"int8\", 'ind_valo_fin_ult1':\"int8\", 'ind_viv_fin_ult1':\"int8\", 'ind_recibo_ult1':\"int8\"}, usecols=usecols)\n",
    "df_t = pd.read_csv('../input/santander-pr/test.csv',dtype={\"sexo\":str,\"ind_nuevo\":str,\"ult_fec_cli_1t\":str,\"indext\":str,\"ind_empleado\":\"category\",\"ind_nuevo\":\"category\",\"indrel\":\"category\",\"indrel_1mes\":\"category\",\"tiprel_1mes\":\"category\",\"indresi\":\"category\",\"indext\":\"category\",\"conyuemp\":\"category\",\"indfall\":\"category\",\"canal_entrada\":\"category\"}, usecols=usecols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE WE ARE SORTING OUR DATA FOR USING LAGS AND TOGGLE FEATURES\n",
    "\n",
    "df.sort_values(by=['ncodpers','fecha_dato'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PRE-PROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The entire pre-processing is same as last notebook therefore we have put all of it in one cell***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE Dropped around 27734 rows which had all its columns empty.\n",
    "\n",
    "df=df[df['indrel'].notna()]\n",
    "\n",
    "# Here we are converting our string dates into datetime datatype . \n",
    "\n",
    "#TRAIN\n",
    "df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "#TEST\n",
    "df_t[\"fecha_dato\"] = pd.to_datetime(df_t[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df_t[\"fecha_alta\"] = pd.to_datetime(df_t[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "\n",
    "# We are dropping antiguedad which refers to the seniority of a customer but rather we are using\n",
    "# a new column cust_since which is the numeric difference between fecha_alta and fecha_dato since antiguedad had\n",
    "# a lot of weird values.\n",
    "\n",
    "# TRAIN\n",
    "df.drop(['antiguedad'],axis=1,inplace=True)\n",
    "df['cust_since'] = df['fecha_dato'].dt.to_period('M').astype(int) - df['fecha_alta'].dt.to_period('M').astype(int)\n",
    "# TEST\n",
    "df_t.drop(['antiguedad'],axis=1,inplace=True)\n",
    "df_t['cust_since'] = df_t['fecha_dato'].dt.to_period('M').astype(int) - df_t['fecha_alta'].dt.to_period('M').astype(int)\n",
    "\n",
    "# In column indrel_1mes the data was not uniform for similar data, some in integer some in string so we converted\n",
    "# it using a dictionary.\n",
    "map_dict = { 1.0  : \"1\",\n",
    "            \"1.0\" : \"1\",\n",
    "            \"1\"   : \"1\",\n",
    "            \"3.0\" : \"3\",\n",
    "            \"P\"   : \"P\",\n",
    "            3.0   : \"3\",\n",
    "            2.0   : \"2\",\n",
    "            \"3\"   : \"3\",\n",
    "            \"2.0\" : \"2\",\n",
    "            4.0   : \"4\",\n",
    "            \"4.0\" : \"4\",\n",
    "            \"4\"   : \"4\",\n",
    "            \"2\"   : \"2\"}\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "# TEST\n",
    "\n",
    "df_t.indrel_1mes = df_t.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "\n",
    "\n",
    "#We replaced the null values using 1\n",
    "\n",
    "df.indrel_1mes.fillna(\"1\",inplace=True)  # TRAIN\n",
    "df_t.indrel_1mes.fillna(\"1\",inplace=True) # TEST\n",
    "\n",
    "# FILLING OF NULL VALUES\n",
    "\n",
    "# WE HAVE FILLED THE NULL VALUES OF ALL COLUMNS USING THE MODE OF THE COLUMN.\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "df['ind_empleado'].fillna(\"N\",inplace=True)\n",
    "df['pais_residencia'].fillna(\"ES\",inplace=True)\n",
    "df['indresi'].fillna(\"S\",inplace=True)\n",
    "df['indext'].fillna(\"N\",inplace=True)\n",
    "df['indfall'].fillna(\"N\",inplace=True)\n",
    "df['sexo'].fillna(df['sexo'].mode()[0], inplace=True)\n",
    "df['segmento'].fillna(df['segmento'].mode()[0], inplace=True)\n",
    "df['canal_entrada'] = df['canal_entrada'].astype('str')\n",
    "df['canal_entrada'].fillna(\"UNK\",inplace=True)\n",
    "df.tiprel_1mes.fillna(\"A\",inplace=True)\n",
    "df['ind_nomina_ult1'].fillna(0,inplace=True)\n",
    "df['ind_nom_pens_ult1'].fillna(0,inplace=True)\n",
    "df.tiprel_1mes.replace(to_replace='N',value='A',inplace=True)\n",
    "\n",
    "# TEST\n",
    "\n",
    "df_t['ind_empleado'].fillna(\"N\",inplace=True)\n",
    "df_t['pais_residencia'].fillna(\"ES\",inplace=True)\n",
    "df_t['indresi'].fillna(\"S\",inplace=True)\n",
    "df_t['indext'].fillna(\"N\",inplace=True)\n",
    "df_t['indfall'].fillna(\"N\",inplace=True)\n",
    "df_t['sexo'].fillna(df_t['sexo'].mode()[0], inplace=True)\n",
    "df_t['segmento'].fillna(df_t['segmento'].mode()[0], inplace=True)\n",
    "df_t['canal_entrada'] = df_t['canal_entrada'].astype('str')\n",
    "df_t['canal_entrada'].fillna(\"UNK\",inplace=True)\n",
    "df_t.tiprel_1mes.fillna(\"A\",inplace=True)\n",
    "df_t.tiprel_1mes.replace(to_replace='N',value='A',inplace=True)\n",
    "\n",
    "\n",
    "#NOW WE WILL HANDLE THE NUMERICAL COLUMNS OF RENTA,AGE and CUST_SINCE.\n",
    "\n",
    "# FIRST THE RENTA COLUMN.\n",
    "\n",
    "# FOR REPLACING THE NULL VALUES OF RENTA WE GROUP IT WITH THEIR RESPECTIVE PROVINCE'S MEAN.\n",
    "# AFTER THAT WE REMOVE OUTLIERS AT 75% QUANTILE MULTIPLIED BY 4 AND 25% QUANTILE DIVIDED BY 4.\n",
    "# THEN WE STANDARDISE THE COLUMN.\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "df['nomprov'].fillna(df['nomprov'].mode()[0], inplace=True)\n",
    "df['renta']=df[['nomprov','renta']].groupby(['nomprov']).transform(lambda x: x.fillna(x.median())).renta\n",
    "max_out=df.renta.quantile(0.75)*4\n",
    "min_out=df.renta.quantile(0.25)/4\n",
    "df.loc[df['renta']>max_out,'renta']=max_out\n",
    "df.loc[df['renta']<min_out,'renta']=min_out\n",
    "scale=StandardScaler().fit(df[['renta']])\n",
    "df[['renta']]=scale.transform(df[['renta']])\n",
    "\n",
    "# TEST \n",
    "\n",
    "df_t['nomprov'].fillna(df_t['nomprov'].mode()[0], inplace=True)\n",
    "df_t['renta']=df_t[['nomprov','renta']].groupby(['nomprov']).transform(lambda x: x.fillna(x.median())).renta\n",
    "max_out=df_t.renta.quantile(0.75)*4\n",
    "min_out=df_t.renta.quantile(0.25)/4\n",
    "df_t.loc[df_t['renta']>max_out,'renta']=max_out\n",
    "df_t.loc[df_t['renta']<min_out,'renta']=min_out\n",
    "scale=StandardScaler().fit(df_t[['renta']])\n",
    "df_t[['renta']]=scale.transform(df_t[['renta']])\n",
    "\n",
    "\n",
    "# SECOND THE AGE COLUMN\n",
    "# WE REMOVE OUTLIERS OF AGES >95 and <12.\n",
    "# THEN WE USE NORMALIZATION , MIN-MAX TECHNIQUE.\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "df.age = pd.to_numeric(df.age,errors=\"coerce\")\n",
    "max_out=df.age.quantile(0.75)*1.9\n",
    "min_out=df.age.quantile(0.25)/2\n",
    "df.loc[df['age']>max_out,'age']=max_out\n",
    "df.loc[df['age']<min_out,'age']=min_out\n",
    "scale=MinMaxScaler().fit(df[['age']])\n",
    "df[['age']]=scale.transform(df[['age']])\n",
    "\n",
    "# TEST\n",
    "\n",
    "df_t.age = pd.to_numeric(df_t.age,errors=\"coerce\")\n",
    "max_out=df_t.age.quantile(0.75)*1.9\n",
    "min_out=df_t.age.quantile(0.25)/2\n",
    "df_t.loc[df_t['age']>max_out,'age']=max_out\n",
    "df_t.loc[df_t['age']<min_out,'age']=min_out\n",
    "scale=MinMaxScaler().fit(df_t[['age']])\n",
    "df_t[['age']]=scale.transform(df_t[['age']])\n",
    "\n",
    "\n",
    "# THIRD THE CUST_SINCE COLUMN\n",
    "# WE NORMALISE THE COLUMNS, MIN-MAX TECHNIQUE.\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "scale=MinMaxScaler().fit(df[['cust_since']])\n",
    "df[['cust_since']]=scale.transform(df[['cust_since']])\n",
    "\n",
    "# TEST\n",
    "\n",
    "scale=MinMaxScaler().fit(df_t[['cust_since']])\n",
    "df_t[['cust_since']]=scale.transform(df_t[['cust_since']])\n",
    "\n",
    "\n",
    "# NOW WE CHANGE DATA TYPES OF COLUMNS FOR BETTER RAM MANAGEMENT.\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "df.ind_nuevo=df.ind_nuevo.astype(np.float64)\n",
    "df.indrel=df.indrel.astype(np.float64)\n",
    "df.ind_actividad_cliente=df.ind_actividad_cliente.astype(np.int8)\n",
    "df.ind_nuevo=df.ind_nuevo.astype(np.int8)\n",
    "df.indrel=df.indrel.astype(np.int8)\n",
    "df.ind_nomina_ult1=df.ind_nomina_ult1.astype(np.int8)\n",
    "df.ind_nom_pens_ult1=df.ind_nom_pens_ult1.astype(np.int8)\n",
    "df.ind_empleado=df.ind_empleado.astype(\"category\")\n",
    "df.sexo=df.sexo.astype(\"category\")\n",
    "\n",
    "# TEST\n",
    "\n",
    "df_t.ind_nuevo=df_t.ind_nuevo.astype(np.float64)\n",
    "df_t.indrel=df_t.indrel.astype(np.float64)\n",
    "df_t.ind_actividad_cliente=df_t.ind_actividad_cliente.astype(np.int8)\n",
    "df_t.ind_nuevo=df_t.ind_nuevo.astype(np.int8)\n",
    "df_t.indrel=df_t.indrel.astype(np.int8)\n",
    "df_t.ind_empleado=df_t.ind_empleado.astype(\"category\")\n",
    "df_t.sexo=df_t.sexo.astype(\"category\")\n",
    "\n",
    "\n",
    "# IN MANY COLUMNS,WE HAD A LOT OF FEATURES AND IT WAS DIFFICULT TO ONE-HOT ENCODE THEM SO WE REDUCED THE NUMBER\n",
    "# OF FEATURES IN COLUMN TO BE FINITE IN 2 COLUMNS.\n",
    "\n",
    "# COLUMN CANAL_ENTRADA\n",
    "# WE REDUCED THE NUMBER OF FEATURES TO BE THE 10 MOST POPULAR FEATURES AND ALL THE REST TO BE IN OTH.\n",
    "\n",
    "canal_name={'KHE' : 'KHE',\n",
    "            'KAT' : 'KAT',\n",
    "            'KFC' : 'KFC',\n",
    "            'KHQ' : 'KHQ',\n",
    "            'KFA' : 'KFA',\n",
    "            'KHK' : 'KHK',\n",
    "            'KHM' : 'KHM',\n",
    "            'UNK' : 'UNK',\n",
    "            'KHD' : 'KHD',\n",
    "            'KHN' : 'KHN'}\n",
    "arr=list(df['canal_entrada'].unique())\n",
    "\n",
    "for name in arr:\n",
    "    if(name not in canal_name.keys()):\n",
    "        canal_name[name]=\"OTH\"\n",
    "\n",
    "arr=list(df_t['canal_entrada'].unique()) \n",
    "\n",
    "for name in arr:\n",
    "    if(name not in canal_name.keys()):\n",
    "        canal_name[name]=\"OTH\"\n",
    "\n",
    "df.canal_entrada = df.canal_entrada.apply(lambda x: canal_name.get(x,x))  # TRAIN\n",
    "\n",
    "df_t.canal_entrada = df_t.canal_entrada.apply(lambda x: canal_name.get(x,x))  # TEST\n",
    "\n",
    "\n",
    "# COLUMN NOMPROV\n",
    "# WE REDUCED THE NUMBER OF FEATURES TO BE THE 10 MOST POPULAR FEATURES AND ALL THE REST TO BE IN OTH.\n",
    "\n",
    "city_name={'MADRID' : 'MADRID',\n",
    "           'BARCELONA' : 'BARCELONA',\n",
    "           'VALENCIA' : 'VALENCIA',\n",
    "           'SEVILLA' : 'SEVILLA',\n",
    "           'CORUÃ‘A, A' : 'CORUNA',\n",
    "           'MURCIA' : 'MURCIA',\n",
    "           'MALAGA' : 'MALAGA',\n",
    "           'ZARAGOZA' : 'ZARAGOZA',\n",
    "           'ALICANTE' : 'ALICANTE',\n",
    "           'CADIZ' : 'CADIZ'}\n",
    "arr=list(df.nomprov.unique())\n",
    "for name in arr:\n",
    "    if(name not in city_name.keys()):\n",
    "        city_name[name]=\"OTH\"\n",
    "        \n",
    "arr=list(df_t.nomprov.unique())\n",
    "for name in arr:\n",
    "    if(name not in city_name.keys()):\n",
    "        city_name[name]=\"OTH\"\n",
    "        \n",
    "df.nomprov = df.nomprov.apply(lambda x: city_name.get(x,x)) # TRAIN\n",
    "\n",
    "df_t.nomprov = df_t.nomprov.apply(lambda x: city_name.get(x,x)) # TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ENCODING AND MAKING OF DATAFRAMES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING OF df_train consisting of all feature columns from df dataframe. \n",
    "\n",
    "df_train = df.loc[:,'fecha_dato':'segmento']\n",
    "df_train['cust_since']=df['cust_since']\n",
    "\n",
    "# NOW WE DROP ANOTHER 6 COLUMNS BECAUSE THEY SHOWED NO IMPORTANCE IN OUR PLOT IMPORTANCE GRAPH.\n",
    "\n",
    "drop=['ind_empleado','pais_residencia','indrel_1mes','indrel','indresi','indfall']\n",
    "df_train.drop(columns=drop,inplace=True)\n",
    "df_t.drop(columns=drop,inplace=True)\n",
    "\n",
    "# NOW WE DO ONE-HOT ENCODING ON OUR FEATURE COLUMNS.\n",
    "\n",
    "df_train=pd.get_dummies(df_train,columns=['nomprov','sexo','ind_nuevo','indext','canal_entrada','ind_actividad_cliente','segmento','tiprel_1mes'])\n",
    "\n",
    "df_test=pd.get_dummies(df_t,columns=['nomprov','sexo','ind_nuevo','indext','canal_entrada','ind_actividad_cliente','segmento','tiprel_1mes'])\n",
    "\n",
    "# df_y consists of all the target columns and few other important columns of which we find the lags and toggles.\n",
    "\n",
    "df_y = df.loc[:,'ind_ahor_fin_ult1':'ind_recibo_ult1']\n",
    "df_y['fecha_dato']=df['fecha_dato']\n",
    "df_y['ncodpers']=df['ncodpers'] \n",
    "df_y['ind_actividad_cliente']=df['ind_actividad_cliente']\n",
    "df_y['segmento']=df['segmento']\n",
    "\n",
    "\n",
    "# NOW We have 3 dataframes df_train,df_test and df_y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **PREPARING DATAFRAMES WHICH WE WILL MERGE WITH TEST DATA**\n",
    "2. **STORING df_lag for all the lags that we will use ahead**\n",
    "3. **Deleting df and df_t for RAM MANAGEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cliente=df[['fecha_dato','ncodpers','ind_actividad_cliente']]\n",
    "df_cliente = df_cliente[df_train['fecha_dato']==\"2015-11-28\"]\n",
    "\n",
    "df_segmento=df[['fecha_dato','ncodpers','segmento']]\n",
    "df_segmento = df_segmento[df_train['fecha_dato']==\"2015-11-28\"]\n",
    "\n",
    "df_lag = df[usecols2]\n",
    "\n",
    "del(df)\n",
    "del(df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REDUCING TIMEFRAME TO ONLY THE REQUIRED TIMEFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[((df_train['fecha_dato']==\"2015-10-28\")| (df_train['fecha_dato']==\"2016-04-28\"))]\n",
    "df_y = df_y[(df_y['fecha_dato']==\"2015-10-28\") | (df_y['fecha_dato']==\"2016-04-28\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDING THE 2 NON-PRODUCT LAGS TO BOTH TEST AND TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "df_y.loc[df_y.ncodpers==df_y.ncodpers.shift(1),'ind_actividad_cliente'+'_lag']=df_y['ind_actividad_cliente'].shift(1)\n",
    "df_y.loc[df_y.ncodpers==df_y.ncodpers.shift(1),'segmento'+'_lag']=df_y['segmento'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAMING BEFORE MERGE\n",
    "\n",
    "df_cliente.rename(columns={'ind_actividad_cliente': 'ind_actividad_cliente' + '_lag'},inplace=True)\n",
    "df_segmento.rename(columns={'segmento': 'segmento' + '_lag'},inplace=True)\n",
    "\n",
    "# TEST\n",
    "\n",
    "df_test=df_test.merge(df_cliente.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "df_test=df_test.merge(df_segmento.loc[:, 'ncodpers':], on='ncodpers', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REDUCING TIMELINE TO JUST TRAINING MONTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[((df_train['fecha_dato']==\"2016-04-28\"))]\n",
    "df_y = df_y[(df_y['fecha_dato']==\"2016-04-28\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FORMING DATAFRAME FOR LAGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORING 13 DATAFRAMES (12 EACH FOR TRAIN AND TEST)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORING LAGS FOR 12 MONTHS\n",
    "\n",
    "df1 = df_lag[df_lag.fecha_dato == \"2016-04-28\"]\n",
    "\n",
    "df2 = df_lag[df_lag.fecha_dato == \"2016-03-28\"]\n",
    "\n",
    "df3 = df_lag[df_lag.fecha_dato == \"2016-02-28\"]\n",
    "\n",
    "df4 = df_lag[df_lag.fecha_dato == \"2016-01-28\"]\n",
    "\n",
    "df5 = df_lag[df_lag.fecha_dato == \"2015-12-28\"]\n",
    "\n",
    "df6 = df_lag[df_lag.fecha_dato == \"2015-11-28\"]\n",
    "\n",
    "df7 = df_lag[df_lag.fecha_dato == \"2015-10-28\"]\n",
    "\n",
    "df8 = df_lag[df_lag.fecha_dato == \"2015-09-28\"]\n",
    "\n",
    "df9 = df_lag[df_lag.fecha_dato == \"2015-08-28\"]\n",
    "\n",
    "df10 = df_lag[df_lag.fecha_dato == \"2015-07-28\"]\n",
    "\n",
    "df11 = df_lag[df_lag.fecha_dato == \"2015-06-28\"]\n",
    "\n",
    "df12 = df_lag[df_lag.fecha_dato == \"2015-05-28\"]\n",
    "\n",
    "df13 = df_lag[df_lag.fecha_dato == \"2015-04-28\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGING OF LAG DATAFRAMES TO TRAIN DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIRST RENAMING BEFORE MERGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df2.columns[2:]:\n",
    "    df2.rename(columns={c:c+\"_lag1\"}, inplace=True)\n",
    "    df3.rename(columns={c:c+\"_lag2\"}, inplace=True)\n",
    "    df4.rename(columns={c:c+\"_lag3\"}, inplace=True)\n",
    "    df5.rename(columns={c:c+\"_lag4\"}, inplace=True)\n",
    "    df6.rename(columns={c:c+\"_lag5\"}, inplace=True)\n",
    "    df7.rename(columns={c:c+\"_lag6\"}, inplace=True)\n",
    "    df8.rename(columns={c:c+\"_lag7\"}, inplace=True)\n",
    "    df9.rename(columns={c:c+\"_lag8\"}, inplace=True)\n",
    "    df10.rename(columns={c:c+\"_lag9\"}, inplace=True)\n",
    "    df11.rename(columns={c:c+\"_lag10\"}, inplace=True)\n",
    "    df12.rename(columns={c:c+\"_lag11\"}, inplace=True)\n",
    "    df13.rename(columns={c:c+\"_lag12\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGE TO TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING 12 MONTH LAGS TO TRAIN DATA\n",
    "\n",
    "df_y=df_y.merge(df2.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df3.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df4.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df5.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df6.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df7.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df8.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df9.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df10.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df11.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df12.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_y=df_y.merge(df13.loc[:, 'ncodpers':], on='ncodpers', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGE OF LAG DATA FRAMES TO TEST DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RENAMING COLUMNS BEFORE MERGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_lag.columns[2:]:\n",
    "    df1.rename(columns={c:c+\"_lag1\"}, inplace=True)\n",
    "    df2.rename(columns={c+\"_lag1\":c+\"_lag2\"}, inplace=True)\n",
    "    df3.rename(columns={c+\"_lag2\":c+\"_lag3\"}, inplace=True)\n",
    "    df4.rename(columns={c+\"_lag3\":c+\"_lag4\"}, inplace=True)\n",
    "    df5.rename(columns={c+\"_lag4\":c+\"_lag5\"}, inplace=True)\n",
    "    df6.rename(columns={c+\"_lag5\":c+\"_lag6\"}, inplace=True)\n",
    "    df7.rename(columns={c+\"_lag6\":c+\"_lag7\"}, inplace=True)\n",
    "    df8.rename(columns={c+\"_lag7\":c+\"_lag8\"}, inplace=True)\n",
    "    df9.rename(columns={c+\"_lag8\":c+\"_lag9\"}, inplace=True)\n",
    "    df10.rename(columns={c+\"_lag9\":c+\"_lag10\"}, inplace=True)\n",
    "    df11.rename(columns={c+\"_lag10\":c+\"_lag11\"}, inplace=True)\n",
    "    df12.rename(columns={c+\"_lag11\":c+\"_lag12\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGE TO TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING 12 MONTHS LAG INTO TEST DATA\n",
    "\n",
    "df_test=df_test.merge(df1.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df2.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df3.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df4.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df5.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df6.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df7.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df8.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df9.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df10.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df11.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "\n",
    "df_test=df_test.merge(df12.loc[:, 'ncodpers':], on='ncodpers', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DELETING ALL THE DATAFRAMES FOR BETTER RAM MANAGEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df2)\n",
    "del(df3)\n",
    "del(df4)\n",
    "del(df5)\n",
    "del(df6)\n",
    "del(df7)\n",
    "del(df8)\n",
    "del(df9)\n",
    "del(df10)\n",
    "del(df11)\n",
    "del(df12)\n",
    "del(df13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVING NULL VALUES AND OTHER PROCESSING IN THE NEWLY CREATED COLUMNS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DATA\n",
    "\n",
    "df_y.fillna(0,inplace=True)\n",
    "df_y.loc[df_y.ind_actividad_cliente_lag.isnull()==True,'ind_actividad_cliente_lag']=df_y.ind_actividad_cliente\n",
    "df_y.loc[df_y.segmento_lag.isnull()==True,'segmento_lag']=df_y.segmento\n",
    "\n",
    "# TEST DATA\n",
    "\n",
    "df_test.loc[(df_test.ind_actividad_cliente_lag.isnull()==True) & (df_test.ind_actividad_cliente_0==1),'ind_actividad_cliente_lag']=0\n",
    "df_test.loc[(df_test.ind_actividad_cliente_lag.isnull()==True) & (df_test.ind_actividad_cliente_1==1),'ind_actividad_cliente_lag']=1\n",
    "df_test.loc[(df_test.segmento_lag.isnull()==True) & (df_test['segmento_01 - TOP']==1) ,'segmento_lag']='TOP'\n",
    "df_test.loc[(df_test.segmento_lag.isnull()==True) & (df_test['segmento_02 - PARTICULARES']==1) ,'segmento_lag']='PARTICULARES'\n",
    "df_test.loc[(df_test.segmento_lag.isnull()==True) & (df_test['segmento_03 - UNIVERSITARIO']==1) ,'segmento_lag']='UNIVERSITARIO'\n",
    "df_test.fillna(0,inplace=True)\n",
    "\n",
    "# CHANGING DATA TYPES\n",
    "\n",
    "#TRAIN\n",
    "\n",
    "for c in df_y.columns[78:]:\n",
    "    df_y[c]=df_y[c].astype(\"int8\")\n",
    "\n",
    "#TEST\n",
    "\n",
    "for c in df_test.columns[44:379]:\n",
    "    df_test[c]=df_test[c].astype(\"int8\")\n",
    "\n",
    "# CONVERTING STRING DATA INTO NUMERICALS.\n",
    "\n",
    "convert={\"02 - PARTICULARES\" : 1,\n",
    "         \"PARTICULARES\" : 1,\n",
    "         \"03 - UNIVERSITARIO\" : 2,\n",
    "         \"UNIVERSITARIO\" : 2,\n",
    "         \"01 - TOP\" : 3,\n",
    "         \"TOP\" : 3}\n",
    "\n",
    "df_y.segmento_lag = df_y.segmento_lag.apply(lambda x: convert.get(x,x))\n",
    "\n",
    "df_test.segmento_lag = df_test.segmento_lag.apply(lambda x: convert.get(x,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME OTHER WORK BEFORE TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHIFTING OF ALL LAG COLUMNS FROM df_y to df_train and dropping them from df_y\n",
    "\n",
    "mer=df_y[df_y.columns[25:]]\n",
    "\n",
    "df_train=df_train.merge(mer,on='ncodpers',how='left')\n",
    "\n",
    "for c in df_y.columns[26:]:\n",
    "    df_y.drop(columns=c,inplace=True)\n",
    "    \n",
    "# REALIIGNING OF TRAIN AND TEST COLUMNS\n",
    "\n",
    "col=df_test.columns\n",
    "df_train=df_train[col]\n",
    "\n",
    "# MAKING OF THE result dataframe.\n",
    "\n",
    "result_curr=pd.DataFrame()\n",
    "result_curr['ncodpers']=df_test['ncodpers']\n",
    "\n",
    "# DROPPING OF COLUMNS WHICH ARE NOT NEEDED IN TRAINING PHASE.\n",
    "\n",
    "df_train.drop(columns=['fecha_dato','ncodpers', 'fecha_alta'], inplace=True)\n",
    "df_test.drop(columns=['fecha_dato','ncodpers', 'fecha_alta'],inplace=True)\n",
    "df_y.drop(columns=['fecha_dato'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(          max_depth=11,\n",
    "                                   n_estimators=700,\n",
    "                                   learning_rate=0.026,\n",
    "                                   num_leaves=105,\n",
    "                                   min_child_weight=25,\n",
    "                                   colsample_bytree=0.42,\n",
    "                                   objective='binary',\n",
    "                                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING FOR OUR DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_y.columns[:-1]:\n",
    "    y_train = df_y[c]\n",
    "    clf.fit(df_train, y_train)\n",
    "    p_train=clf.predict_proba(df_test)[:,1]\n",
    "    result_curr[c]=p_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ENSEMBLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READING THE OTHER NOTEBOOK'S RESULT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_old=pd.read_csv('../input/old-result/result_old.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame()\n",
    "result['ncodpers']=result_curr['ncodpers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WE WILL ENSEMBLE result_curr and result_old**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in result_curr.columns[1:]:\n",
    "    result[c]=((0.58*result_curr[c]) + (0.42*result_old[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONVERSION TO MAP5 FORMAT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.merge(df1.loc[:, 'ncodpers':], on='ncodpers', how='left')\n",
    "result.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in result_curr.columns[1:]:\n",
    "    result[c+'_ans']=abs(result[c]-result[c+'_lag1'])\n",
    "finalresult=pd.DataFrame(columns=['ncodpers', 'changed'])\n",
    "finalresult['ncodpers']=result['ncodpers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes=[]\n",
    "for i in range(len(result)):\n",
    "    toadd=[]\n",
    "    for j in result.columns[1:25]:\n",
    "        toadd.append([result[j+\"_ans\"][i], j])\n",
    "    toadd.sort(key=lambda x: -x[0])\n",
    "    toadd=toadd[0:5]\n",
    "    toadd1=\"\"\n",
    "    for j in toadd:\n",
    "        toadd1=toadd1+j[1]+\" \"\n",
    "    yes.append(toadd1)\n",
    "finalresult['changed']=yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresult.to_csv('FinalResult.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
